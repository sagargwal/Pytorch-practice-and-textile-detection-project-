{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0.],\n",
       "          [0., 0.]],\n",
       "\n",
       "         [[0., 0.],\n",
       "          [0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0.],\n",
       "          [0., 0.]],\n",
       "\n",
       "         [[0., 0.],\n",
       "          [0., 0.]]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(2,2,2,2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4113, 0.2855],\n",
       "        [0.5292, 0.9019]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.rand(2,2)\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "#  to know data type \n",
    "print(x1.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to change datatype\n",
    "x1 = torch.rand(2,2,dtype = torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.HalfTensor'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2000, 4.5000],\n",
       "        [1.2000, 8.9000]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## creating tensor froma list \n",
    "x2 = torch.tensor([[1.2,4.5],[1.2,8.9]])\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = torch.tensor([1.2,4.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2000, 4.5000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3539, 0.4765],\n",
       "         [0.7220, 0.3016]]),\n",
       " tensor([[0.9845, 0.8872],\n",
       "         [0.6160, 0.2810]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# addition of tensors\n",
    "x = torch.rand(2,2)\n",
    "y = torch.rand(2,2)\n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3384, 1.3637],\n",
       "        [1.3380, 0.5825]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3384, 1.3637],\n",
       "        [1.3380, 0.5825]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9845, -0.8872],\n",
       "        [-0.6160, -0.2810]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  substraction in pytorch\n",
    "x-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9845, -0.8872],\n",
       "        [-0.6160, -0.2810]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sub(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4737, 0.6497],\n",
       "        [0.9660, 0.1757]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mutiply of tensors\n",
    "torch.mul(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4737, 0.6497],\n",
       "        [0.9660, 0.1757]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mul_(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7471, 0.7333],\n",
       "        [0.7474, 1.7167]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# divding two tensors\n",
    "torch.div(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7471, 0.7333],\n",
       "        [0.7474, 1.7167]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x/y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6915, 0.5756, 0.0617],\n",
       "        [0.1396, 0.6597, 0.0081],\n",
       "        [0.1962, 0.5607, 0.5564],\n",
       "        [0.3767, 0.0938, 0.5097],\n",
       "        [0.5012, 0.6739, 0.7444]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slicing with tensors \n",
    "eg = torch.rand(5,3)\n",
    "eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6915, 0.1396, 0.1962, 0.3767, 0.5012])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  to get first column \n",
    "eg[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6915, 0.5756, 0.0617])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get first row \n",
    "eg[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0081)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get a specific value \n",
    "eg[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008093476295471191"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get the actual value\n",
    "eg[1,2].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9606, 0.7351],\n",
       "         [0.8130, 0.3825]],\n",
       "\n",
       "        [[0.1643, 0.3785],\n",
       "         [0.3250, 0.3645]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  now lets experiment if the tensor is three d\n",
    "threeD = torch.rand([2,2,2])\n",
    "threeD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3785)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  first is which 2d matrix and second and third are same as 2d ,here first tensor(not zeroth) 0 row 1 column\n",
    "threeD[1,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6915, 0.5756, 0.0617],\n",
       "        [0.1396, 0.6597, 0.0081],\n",
       "        [0.1962, 0.5607, 0.5564],\n",
       "        [0.3767, 0.0938, 0.5097],\n",
       "        [0.5012, 0.6739, 0.7444]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to reshape a tensor \n",
    "eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6915, 0.5756, 0.0617, 0.1396, 0.6597, 0.0081, 0.1962, 0.5607, 0.5564,\n",
       "        0.3767, 0.0938, 0.5097, 0.5012, 0.6739, 0.7444])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  to reshape in a tensor into 1D\n",
    "oneD = eg.view(15)\n",
    "oneD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  -1 in row automatically takes the best shape for tensor \n",
    "fivexthree = oneD.view(-1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fivexthree.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourxfive = torch.rand(4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiveXfour = fourxfive.view(5,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0722, 0.5762, 0.5604, 0.3552],\n",
       "        [0.9133, 0.9192, 0.8321, 0.9994],\n",
       "        [0.9557, 0.6377, 0.0722, 0.1509],\n",
       "        [0.4882, 0.2923, 0.3175, 0.4029],\n",
       "        [0.0588, 0.4117, 0.5265, 0.2860]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiveXfour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiveXfour.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3., 3., 3., 3., 3.]), array([3., 3., 3., 3., 3.], dtype=float32))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert tensor into numpy array \n",
    "#  as it is require many function takes np.arraya as input and not tensors\n",
    "import numpy as np \n",
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "#  but here both share same location in the memory so any change is a will cause the same cahnge in b\n",
    "a.add_(2)\n",
    "a,b\n",
    "\n",
    "#  both are changed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  to convert numpy array into tensor \n",
    "aa = np.ones([5,2])\n",
    "bb = torch.from_numpy(aa)\n",
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  to check cuda is available or gpu is available \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    xg = torch.ones(5,device=device)\n",
    "xg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate gradient with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0056, -1.1897, -0.4277], requires_grad=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eg1 = torch.randn(3, requires_grad = True)\n",
    "eg1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.9944, 0.8103, 1.5723], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eg1\n",
    "y1 = eg1+2\n",
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.9556, 1.3132, 4.9441], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y1*y1*2\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2000, 3.3000, 4.5000])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v =  torch.tensor([1.2,3.3,4.5], dtype = torch.float32)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward(v) #claculating   z product v lets say g and than dg/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9.5733, 10.6959, 28.3009])\n"
     ]
    }
   ],
   "source": [
    "print(eg1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  to let the tensor to not calulate gradient \n",
    "#  use x.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.ones(4, requires_grad = True)\n",
    "\n",
    "# for epoch in range(3):\n",
    "#     model_output = (weights*3).sum()\n",
    "#     model_output.backward()\n",
    "#     print(weights.grad)\n",
    "#     weights.grad.zero_()\n",
    "\n",
    "#  the reason why grad.zero() is used is to take the gradient to nonnnnne again cause here      it is good enough but is \n",
    "#  some cases where x is changing say in graient optimitation there with every epoch gradiant should start from zero "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.SGD([weights], lr=0.01)\n",
    "\n",
    "# Define a simple loss function\n",
    "loss = (weights * 2).sum()  # Example: loss = sum(weights * 2)\n",
    "\n",
    "loss.backward()  # Compute gradients\n",
    "\n",
    "optimizer.step()  # Update weights\n",
    "\n",
    "print(weights.grad)  # Check updated weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Gradients: tensor([2., 2., 2., 2.])\n",
      "  Weights before update: tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "  Weights after update: tensor([0.9800, 0.9800, 0.9800, 0.9800], requires_grad=True)\n",
      "  Loss: 4.0\n",
      "\n",
      "Epoch 2:\n",
      "  Gradients: tensor([1.9600, 1.9600, 1.9600, 1.9600])\n",
      "  Weights before update: tensor([0.9800, 0.9800, 0.9800, 0.9800], requires_grad=True)\n",
      "  Weights after update: tensor([0.9604, 0.9604, 0.9604, 0.9604], requires_grad=True)\n",
      "  Loss: 3.841600179672241\n",
      "\n",
      "Epoch 3:\n",
      "  Gradients: tensor([1.9208, 1.9208, 1.9208, 1.9208])\n",
      "  Weights before update: tensor([0.9604, 0.9604, 0.9604, 0.9604], requires_grad=True)\n",
      "  Weights after update: tensor([0.9412, 0.9412, 0.9412, 0.9412], requires_grad=True)\n",
      "  Loss: 3.6894729137420654\n",
      "\n",
      "Epoch 4:\n",
      "  Gradients: tensor([1.8824, 1.8824, 1.8824, 1.8824])\n",
      "  Weights before update: tensor([0.9412, 0.9412, 0.9412, 0.9412], requires_grad=True)\n",
      "  Weights after update: tensor([0.9224, 0.9224, 0.9224, 0.9224], requires_grad=True)\n",
      "  Loss: 3.543369770050049\n",
      "\n",
      "Epoch 5:\n",
      "  Gradients: tensor([1.8447, 1.8447, 1.8447, 1.8447])\n",
      "  Weights before update: tensor([0.9224, 0.9224, 0.9224, 0.9224], requires_grad=True)\n",
      "  Weights after update: tensor([0.9039, 0.9039, 0.9039, 0.9039], requires_grad=True)\n",
      "  Loss: 3.4030520915985107\n",
      "\n",
      "Epoch 6:\n",
      "  Gradients: tensor([1.8078, 1.8078, 1.8078, 1.8078])\n",
      "  Weights before update: tensor([0.9039, 0.9039, 0.9039, 0.9039], requires_grad=True)\n",
      "  Weights after update: tensor([0.8858, 0.8858, 0.8858, 0.8858], requires_grad=True)\n",
      "  Loss: 3.268291473388672\n",
      "\n",
      "Epoch 7:\n",
      "  Gradients: tensor([1.7717, 1.7717, 1.7717, 1.7717])\n",
      "  Weights before update: tensor([0.8858, 0.8858, 0.8858, 0.8858], requires_grad=True)\n",
      "  Weights after update: tensor([0.8681, 0.8681, 0.8681, 0.8681], requires_grad=True)\n",
      "  Loss: 3.1388673782348633\n",
      "\n",
      "Epoch 8:\n",
      "  Gradients: tensor([1.7363, 1.7363, 1.7363, 1.7363])\n",
      "  Weights before update: tensor([0.8681, 0.8681, 0.8681, 0.8681], requires_grad=True)\n",
      "  Weights after update: tensor([0.8508, 0.8508, 0.8508, 0.8508], requires_grad=True)\n",
      "  Loss: 3.014568328857422\n",
      "\n",
      "Epoch 9:\n",
      "  Gradients: tensor([1.7015, 1.7015, 1.7015, 1.7015])\n",
      "  Weights before update: tensor([0.8508, 0.8508, 0.8508, 0.8508], requires_grad=True)\n",
      "  Weights after update: tensor([0.8337, 0.8337, 0.8337, 0.8337], requires_grad=True)\n",
      "  Loss: 2.895191192626953\n",
      "\n",
      "Epoch 10:\n",
      "  Gradients: tensor([1.6675, 1.6675, 1.6675, 1.6675])\n",
      "  Weights before update: tensor([0.8337, 0.8337, 0.8337, 0.8337], requires_grad=True)\n",
      "  Weights after update: tensor([0.8171, 0.8171, 0.8171, 0.8171], requires_grad=True)\n",
      "  Loss: 2.7805416584014893\n",
      "\n",
      "Final optimized weights: tensor([0.8171, 0.8171, 0.8171, 0.8171], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "weights = torch.ones(4, requires_grad=True)  # Initialize weights as [1,1,1,1] with gradient tracking\n",
    "optimizer = torch.optim.SGD([weights], lr=0.01)  # Define the optimizer with learning rate 0.01\n",
    "\n",
    "for epoch in range(10):  # Run multiple optimization steps\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    loss = (weights ** 2).sum()  # Compute loss\n",
    "    loss.backward()  # Compute gradient (dloss/dw)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}:\")\n",
    "    print(f\"  Gradients: {weights.grad}\")  # Print gradients before updating weights\n",
    "    print(f\"  Weights before update: {weights}\")\n",
    "\n",
    "    optimizer.step()  # Update weights using gradients\n",
    "\n",
    "    print(f\"  Weights after update: {weights}\")\n",
    "    print(f\"  Loss: {loss.item()}\\n\")  # Print loss\n",
    "\n",
    "print(\"Final optimized weights:\", weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 2: w = 1.680, loss = 4.79999924\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 4: w = 1.949, loss = 0.12288000\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 6: w = 1.992, loss = 0.00314574\n",
      "epoch 7: w = 1.997, loss = 0.00050331\n",
      "epoch 8: w = 1.999, loss = 0.00008053\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 10: w = 2.000, loss = 0.00000206\n",
      "epoch 11: w = 2.000, loss = 0.00000033\n",
      "epoch 12: w = 2.000, loss = 0.00000005\n",
      "epoch 13: w = 2.000, loss = 0.00000001\n",
      "epoch 14: w = 2.000, loss = 0.00000000\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "epoch 16: w = 2.000, loss = 0.00000000\n",
      "epoch 17: w = 2.000, loss = 0.00000000\n",
      "epoch 18: w = 2.000, loss = 0.00000000\n",
      "epoch 19: w = 2.000, loss = 0.00000000\n",
      "epoch 20: w = 2.000, loss = 0.00000000\n",
      "prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# optimize gradient with auto grad date - 17/2/2025\n",
    "# making a model with numpy \n",
    "import numpy as np\n",
    "# model\n",
    "# f = w*x\n",
    "x= np.array([1,2,3,4], dtype=np.float32)\n",
    "y = np.array([2,4,6,8], dtype=np.float32)\n",
    "\n",
    "w= 0.0\n",
    "# model prediction \n",
    "def forward(x):\n",
    "    return w*x\n",
    "# loss = MSE\n",
    "def loss(y,y_predicted):\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "# gradient\n",
    "# dj/dw = 1/n 2x (w*x - y)\n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean()\n",
    "print(f\"prediction before training: f(5) = {forward(5):.3f}\")\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_iters):\n",
    "    y_predicted = forward(x)\n",
    "    l = loss(y,y_predicted)\n",
    "    dw = gradient(x,y,y_predicted)\n",
    "    w = w - learning_rate*dw\n",
    "    if epoch %1 == 0:\n",
    "        print(f\"epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}\")\n",
    "    \n",
    "print(f\"prediction after training: f(5) = {forward(5):.3f}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 3: w = 0.772, loss = 15.66018772\n",
      "epoch 5: w = 1.113, loss = 8.17471695\n",
      "epoch 7: w = 1.359, loss = 4.26725292\n",
      "epoch 9: w = 1.537, loss = 2.22753215\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 13: w = 1.758, loss = 0.60698116\n",
      "epoch 15: w = 1.825, loss = 0.31684780\n",
      "epoch 17: w = 1.874, loss = 0.16539653\n",
      "epoch 19: w = 1.909, loss = 0.08633806\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 23: w = 1.952, loss = 0.02352631\n",
      "epoch 25: w = 1.966, loss = 0.01228084\n",
      "epoch 27: w = 1.975, loss = 0.00641066\n",
      "epoch 29: w = 1.982, loss = 0.00334642\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 33: w = 1.991, loss = 0.00091188\n",
      "epoch 35: w = 1.993, loss = 0.00047601\n",
      "epoch 37: w = 1.995, loss = 0.00024848\n",
      "epoch 39: w = 1.996, loss = 0.00012971\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 43: w = 1.998, loss = 0.00003534\n",
      "epoch 45: w = 1.999, loss = 0.00001845\n",
      "epoch 47: w = 1.999, loss = 0.00000963\n",
      "epoch 49: w = 1.999, loss = 0.00000503\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 53: w = 2.000, loss = 0.00000137\n",
      "epoch 55: w = 2.000, loss = 0.00000071\n",
      "epoch 57: w = 2.000, loss = 0.00000037\n",
      "epoch 59: w = 2.000, loss = 0.00000019\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 63: w = 2.000, loss = 0.00000005\n",
      "epoch 65: w = 2.000, loss = 0.00000003\n",
      "epoch 67: w = 2.000, loss = 0.00000001\n",
      "epoch 69: w = 2.000, loss = 0.00000001\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 73: w = 2.000, loss = 0.00000000\n",
      "epoch 75: w = 2.000, loss = 0.00000000\n",
      "epoch 77: w = 2.000, loss = 0.00000000\n",
      "epoch 79: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 83: w = 2.000, loss = 0.00000000\n",
      "epoch 85: w = 2.000, loss = 0.00000000\n",
      "epoch 87: w = 2.000, loss = 0.00000000\n",
      "epoch 89: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "epoch 93: w = 2.000, loss = 0.00000000\n",
      "epoch 95: w = 2.000, loss = 0.00000000\n",
      "epoch 97: w = 2.000, loss = 0.00000000\n",
      "epoch 99: w = 2.000, loss = 0.00000000\n",
      "prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# lets do it again with pytorc and get graidient by auto grad \n",
    "import torch\n",
    "import numpy as np\n",
    "# model\n",
    "# f = w*x\n",
    "x= torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "\n",
    "w= torch.tensor(0.0,dtype = torch.float32, requires_grad = True)\n",
    "# model prediction \n",
    "def forward(x):\n",
    "    return w*x\n",
    "# loss = MSE\n",
    "def loss(y,y_predicted):\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "# gradient\n",
    "# dj/dw = 1/n 2x (w*x - y)\n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean()\n",
    "print(f\"prediction before training: f(5) = {forward(5):.3f}\")\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_iters):\n",
    "    y_predicted = forward(x)\n",
    "    l = loss(y,y_predicted)\n",
    "    # gradients = backward pass\n",
    "    l.backward() #dl/dw\n",
    "    # update weight\n",
    "    with torch.no_grad():\n",
    "        w-= learning_rate * w.grad\n",
    "    # zero gradient or emptying the gradient\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if epoch %2 == 0:\n",
    "        print(f\"epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}\")\n",
    "    \n",
    "print(f\"prediction after training: f(5) = {forward(5):.3f}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "epoch 1: w = 0.884, loss = 14.05496597\n",
      "epoch 11: w = 1.745, loss = 0.37514615\n",
      "epoch 21: w = 1.886, loss = 0.02054649\n",
      "epoch 31: w = 1.911, loss = 0.01074119\n",
      "epoch 41: w = 1.917, loss = 0.00989325\n",
      "prediction after training: f(5) = 9.834\n"
     ]
    }
   ],
   "source": [
    "# using pytorch loss and optimizer class in pytorch \n",
    "# lets do it again with pytorc and get graidient by auto grad \n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "# model\n",
    "# f = w*x\n",
    "x= torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "\n",
    "#  we also need to make a test as f(5) directly can not be called here\n",
    "x_test  = torch.tensor([5], dtype = torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "# w= torch.tensor(0.0,dtype = torch.float32, requires_grad = True)\n",
    "#  do not require w now as nn models arelay have w as a param\n",
    "# we need to defrine number of features and number of sample as here both are equal \n",
    "n_samples, n_features = x.shape\n",
    "print(n_samples,n_features)\n",
    "\n",
    "#  we also need to define the shape of input size and out put size\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "\n",
    "# model prediction \n",
    "# def forward(x):\n",
    "#     return w*x\n",
    "# this will changes by already present linear model\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 50\n",
    "\n",
    "\n",
    "# loss = MSE\n",
    "# def loss(y,y_predicted):\n",
    "#     return ((y_predicted-y)**2).mean()\n",
    "#  nn also has a loss function that can be used \n",
    "loss = nn.MSELoss()\n",
    "\n",
    "\n",
    "# gradient\n",
    "# dj/dw = 1/n 2x (w*x - y)\n",
    "# def gradient(x,y,y_predicted):\n",
    "#     return np.dot(2*x, y_predicted-y).mean()\n",
    "#  torch also has an optimizer function \n",
    "optimizer  =  torch.optim.SGD(model.parameters(),lr = learning_rate)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_iters):\n",
    "    y_predicted = model(x)\n",
    "    \n",
    "    l = loss(y,y_predicted)\n",
    "    \n",
    "    # gradients = backward pass\n",
    "    l.backward() #dl/dw\n",
    "    # update weight\n",
    "    \n",
    "    # with torch.no_grad():\n",
    "    #     w-= learning_rate * w.grad\n",
    "    # THIS CAN BE DONE BY OPTIMIZER\n",
    "    optimizer.step()\n",
    "    \n",
    "    # zero gradient or emptying the gradient\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch %10 == 0:\n",
    "        [w,b] = model.parameters()\n",
    "        print(f\"epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}\")\n",
    "    \n",
    "print(f\"prediction after training: f(5) = {model(x_test).item():.3f}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3090990130.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[52], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    def __init__(self.input_dim, output_dim):\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#  a custum linear regression\n",
    "class LinearRegression(nn.Module):\n",
    "     def __init__(self,input_dim, output_dim):\n",
    "         super(LinearRegression, self).__init__()\n",
    "         self.lin = nn.Linear(input_dim, output_dim)\n",
    "     def forward(self,x):\n",
    "         return self.lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "epoch 1: w = 0.357, loss = 23.70212173\n",
      "epoch 11: w = 1.478, loss = 0.74837607\n",
      "epoch 21: w = 1.666, loss = 0.14664222\n",
      "epoch 31: w = 1.703, loss = 0.12366562\n",
      "epoch 41: w = 1.716, loss = 0.11609395\n",
      "prediction after training: f(5) = 9.432\n"
     ]
    }
   ],
   "source": [
    "# using pytorch loss and optimizer class in pytorch \n",
    "# lets do it again with pytorc and get graidient by auto grad \n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "# model\n",
    "# f = w*x\n",
    "x= torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "\n",
    "#  we also need to make a test as f(5) directly can not be called here\n",
    "x_test  = torch.tensor([5], dtype = torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "# w= torch.tensor(0.0,dtype = torch.float32, requires_grad = True)\n",
    "#  do not require w now as nn models arelay have w as a param\n",
    "# we need to defrine number of features and number of sample as here both are equal \n",
    "n_samples, n_features = x.shape\n",
    "print(n_samples,n_features)\n",
    "\n",
    "#  we also need to define the shape of input size and out put size\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "\n",
    "# model prediction \n",
    "# def forward(x):\n",
    "#     return w*x\n",
    "# this will changes by already present linear model\n",
    "#  a custum linear regression\n",
    "class LinearRegression(nn.Module):\n",
    "     def __init__(self, input_dim, output_dim):\n",
    "         super(LinearRegression, self).__init__()\n",
    "         self.lin = nn.Linear(input_dim, output_dim)\n",
    "     def forward(self,x):\n",
    "         return self.lin(x)\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 50\n",
    "\n",
    "\n",
    "# loss = MSE\n",
    "# def loss(y,y_predicted):\n",
    "#     return ((y_predicted-y)**2).mean()\n",
    "#  nn also has a loss function that can be used \n",
    "loss = nn.MSELoss()\n",
    "\n",
    "\n",
    "# gradient\n",
    "# dj/dw = 1/n 2x (w*x - y)\n",
    "# def gradient(x,y,y_predicted):\n",
    "#     return np.dot(2*x, y_predicted-y).mean()\n",
    "#  torch also has an optimizer function \n",
    "optimizer  =  torch.optim.SGD(model.parameters(),lr = learning_rate)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(n_iters):\n",
    "    y_predicted = model(x)\n",
    "    \n",
    "    l = loss(y,y_predicted)\n",
    "    \n",
    "    # gradients = backward pass\n",
    "    l.backward() #dl/dw\n",
    "    # update weight\n",
    "    \n",
    "    # with torch.no_grad():\n",
    "    #     w-= learning_rate * w.grad\n",
    "    # THIS CAN BE DONE BY OPTIMIZER\n",
    "    optimizer.step()\n",
    "    \n",
    "    # zero gradient or emptying the gradient\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch %10 == 0:\n",
    "        [w,b] = model.parameters()\n",
    "        print(f\"epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}\")\n",
    "    \n",
    "print(f\"prediction after training: f(5) = {model(x_test).item():.3f}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10,loss = 4473.2837\n",
      "epoch: 20,loss = 3337.2793\n",
      "epoch: 30,loss = 2514.7898\n",
      "epoch: 40,loss = 1918.6831\n",
      "epoch: 50,loss = 1486.2423\n",
      "epoch: 60,loss = 1172.2577\n",
      "epoch: 70,loss = 944.0984\n",
      "epoch: 80,loss = 778.1817\n",
      "epoch: 90,loss = 657.4458\n",
      "epoch: 100,loss = 569.5328\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt \n",
    "# prepare data \n",
    "x_numpy, y_numpy  = datasets.make_regression(n_samples = 100,n_features = 1, noise = 20, random_state =1)\n",
    "\n",
    "x = torch.from_numpy(x_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "y = y.view(y.shape[0],1)\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "# 1) model\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "# )loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer =  torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "# 3) training loop \n",
    "num_epoch = 100\n",
    "for epoch in range(num_epoch):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred,y)\n",
    "\n",
    "    # backwardpass\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"epoch: {epoch+1},loss = {loss.item():.4f}\")\n",
    "predicted  = model(x).detach().numpy()\n",
    "plt.plot(x_numpy, y_numpy, \"ro\")\n",
    "plt.plot(x_numpy, predicted, \"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10,loss = 0.5063\n",
      "epoch: 20,loss = 0.4198\n",
      "epoch: 30,loss = 0.3642\n",
      "epoch: 40,loss = 0.3255\n",
      "epoch: 50,loss = 0.2969\n",
      "epoch: 60,loss = 0.2747\n",
      "epoch: 70,loss = 0.2570\n",
      "epoch: 80,loss = 0.2424\n",
      "epoch: 90,loss = 0.2302\n",
      "epoch: 100,loss = 0.2198\n",
      "accuracy = 0.939\n"
     ]
    }
   ],
   "source": [
    "# logistic regression with pytorch\n",
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "bc = datasets.load_breast_cancer()\n",
    "x, y= bc.data, bc.target\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "# train_test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size= 0.2,random_state=1234)\n",
    "# scale\n",
    "sc= StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)\n",
    "\n",
    "x_train = torch.from_numpy(x_train.astype(np.float32))\n",
    "x_test = torch.from_numpy(x_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0],1)\n",
    "y_test = y_test.view(y_test.shape[0],1)\n",
    "\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self,n_input_features):\n",
    "        super(LogisticRegression,self).__init__()\n",
    "        self.lin = nn.Linear(n_input_features,1)\n",
    "    def forward(self,x):\n",
    "        y_pred = torch.sigmoid(self.lin(x))\n",
    "        return y_pred\n",
    "\n",
    "model = LogisticRegression(n_features)\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "num_epoch = 100\n",
    "for epoch in range(num_epoch):\n",
    "    y_pred = model(x_train)\n",
    "    loss = criterion(y_pred,y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"epoch: {epoch+1},loss = {loss.item():.4f}\")\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(x_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    acc = y_predicted_cls.eq(y_test).sum()/ float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Where:\n",
    "- **logit[0] = 2.0** (for **cat** ),\n",
    "- **logit[1] = 1.0** (for **dog** ),\n",
    "- **logit[2] = 0.1** (for **fish** ).\n",
    "\n",
    "The **true label** (ground truth) for this input image is **cat** (class 0).\n",
    "\n",
    "---\n",
    "\n",
    "##  Step 1: Apply Softmax Function\n",
    "\n",
    "The **softmax function** transforms the logits into probabilities. The formula for softmax for each class \\( i \\) is:\n",
    "\n",
    "<div align=\"center\">\n",
    "  \n",
    "$$\n",
    "P(y_i) = \\frac{e^{\\text{logit}_i}}{\\sum_j e^{\\text{logit}_j}}\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "Where:\n",
    "- \\( P(y_i) \\) is the probability of class \\( i \\),\n",
    "- \\( \\text{logit}_i \\) is the raw logit for class \\( i \\),\n",
    "- The denominator sums over all classes \\( j \\).\n",
    "\n",
    "###  Compute the exponentials of the logits:\n",
    "\n",
    "<div align=\"center\">\n",
    "  \n",
    "$$\n",
    "e^{\\text{logit}_0} = e^{2.0} = 7.389\n",
    "$$\n",
    "\n",
    "$$\n",
    "e^{\\text{logit}_1} = e^{1.0} = 2.718\n",
    "$$\n",
    "\n",
    "$$\n",
    "e^{\\text{logit}_2} = e^{0.1} = 1.105\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "###  Sum of exponentials:\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "$$\n",
    "\\sum e^{\\text{logits}} = 7.389 + 2.718 + 1.105 = 11.212\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "###  Compute the probabilities:\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "$$\n",
    "P(\\text{cat}) = \\frac{e^{2.0}}{\\sum e^{\\text{logits}}} = \\frac{7.389}{11.212} = 0.659\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{dog}) = \\frac{e^{1.0}}{\\sum e^{\\text{logits}}} = \\frac{2.718}{11.212} = 0.242\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{fish}) = \\frac{e^{0.1}}{\\sum e^{\\text{logits}}} = \\frac{1.105}{11.212} = 0.098\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "Now we have the predicted probabilities:\n",
    "\n",
    "\n",
    "###  Compute the cross-entropy loss:\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "$$\n",
    "L = - (t_0 \\log(P_0) + t_1 \\log(P_1) + t_2 \\log(P_2))\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "Since \\( t_1 = 0 \\) and \\( t_2 = 0 \\), those terms vanish:\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "$$\n",
    "L = - (1 \\times \\log(0.659) + 0 \\times \\log(0.242) + 0 \\times \\log(0.098))\n",
    "$$\n",
    "\n",
    "$$\n",
    "L = - \\log(0.659)\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "###  Compute the Logarithm:\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "$$\n",
    "L = -(-0.416) = 0.416\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "##  Final Answer:\n",
    "\n",
    "The **cross-entropy loss** is:0.416\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.4170299470424652\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "#  performing it using pytorch \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import numpy as np \n",
    "# nn.Crossentropy has a built in softmax so we just have to give our logits that came from a neural network so it calculates the probablity by iself\n",
    "loss  = nn.CrossEntropyLoss()\n",
    "# nsamples x nclasses = 1x3\n",
    "y = torch.tensor([0])# this is label represents first class \n",
    "#  y predicted\n",
    "y_pred = torch.tensor([[2.0,1.0,0.1]])\n",
    "l1 = loss(y_pred,y)\n",
    "\n",
    "print(f\"loss = {l1.item()}\")\n",
    "\n",
    "#  prediction meaning which class is it \n",
    "_, prediction = torch.max(y_pred,1)# mean find max in first dimension \n",
    "if prediction.item() ==0:\n",
    "    print(\"cat\")\n",
    "# though loss will be optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for multi class problem a neural netwrok \n",
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self, input_size,hidden_size, num_classes):\n",
    "        super(NeuralNet2,self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "model =  NeuralNet2(input_size= 28*28, hidden_size = 5, num_classes = 3)\n",
    "criterion= nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# making a feed forward neural network\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#  device config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# hyper parameters \n",
    "input_size = 784\n",
    "hidden_size = 100\n",
    "num_classes = 10\n",
    "num_epochs = 2\n",
    "batch_size =100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# minst\n",
    "train_dataset = torchvision.datasets.MNIST(root = \"./data\", train =True, transform = transforms.ToTensor(), download = True)\n",
    "test_dataset = torchvision.datasets.MNIST(root = \"./data\", train = False, transform = transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset= test_dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "examples = iter(train_loader)\n",
    "samples, labels = next(examples)\n",
    "print(samples.size(),labels.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(samples[i][0],cmap = \"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/2, step 100/600, loss = 0.4916\n",
      "epoch 1/2, step 200/600, loss = 0.3487\n",
      "epoch 1/2, step 300/600, loss = 0.3769\n",
      "epoch 1/2, step 400/600, loss = 0.2464\n",
      "epoch 1/2, step 500/600, loss = 0.2032\n",
      "epoch 1/2, step 600/600, loss = 0.3461\n",
      "epoch 2/2, step 100/600, loss = 0.1511\n",
      "epoch 2/2, step 200/600, loss = 0.1552\n",
      "epoch 2/2, step 300/600, loss = 0.2482\n",
      "epoch 2/2, step 400/600, loss = 0.2121\n",
      "epoch 2/2, step 500/600, loss = 0.246\n",
      "epoch 2/2, step 600/600, loss = 0.2053\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m     _, predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     43\u001b[0m     n_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 44\u001b[0m     n_correct \u001b[38;5;241m=\u001b[39m (\u001b[43mprediction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     46\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100.0\u001b[39m \u001b[38;5;241m*\u001b[39mn_correct\u001b[38;5;241m/\u001b[39mn_samples\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_classes):\n",
    "        super(NeuralNet,self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size,hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size,num_classes)\n",
    "    def forward(self,x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out \n",
    "model = NeuralNet(input_size,hidden_size,num_classes)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
    "#  training loop \n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        # forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs,labels)\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "           print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images,labels in test_loader:\n",
    "        images = images.reshape(-1,28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # VALUE , INDEX\n",
    "        _, predictions = torch.max(outputs,1)\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct = (prediction == labels.to(device)).sum().item()\n",
    "\n",
    "    acc = 100.0 *n_correct/n_samples\n",
    "    print(f'accuracy = {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images,labels in test_loader:\n",
    "        images = images.reshape(-1,28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # VALUE , INDEX\n",
    "        _, predictions = torch.max(outputs,1)\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct = (prediction == labels.to(device)).sum().item()\n",
    "\n",
    "    acc = 100.0 *n_correct/n_samples\n",
    "    print(f'accuracy = {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 95.29%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # value, index\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # Update totals\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f\"Accuracy = {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# CNN Architecture: Tensor Shape Changes Step-by-Step\n",
    "\n",
    "## **Step 1: Input Tensor**\n",
    "\n",
    "The input tensor has shape:\n",
    "\n",
    "```\n",
    "[4, 3, 32, 32]\n",
    "```\n",
    "\n",
    "where:\n",
    "- **4**  Batch size (number of images)\n",
    "- **3**  Number of input channels (RGB)\n",
    "- **32  32**  Image dimensions\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 2: First Convolution Layer (conv1)**\n",
    "\n",
    "```python\n",
    "self.conv1 = nn.Conv2d(3,6,5)\n",
    "```\n",
    "\n",
    "- This applies **6 filters** of size **55** with **stride=1, padding=0**.\n",
    "- Formula for output size (H' & W'):\n",
    "$$\n",
    "  \\[ H' = \\frac{H - \\text{kernel size} + 2 \\times \\text{padding}}{\\text{stride}} + 1 \\]\n",
    "  \\[ W' = \\frac{W - \\text{kernel size} + 2 \\times \\text{padding}}{\\text{stride}} + 1 \\]\n",
    "$$\n",
    "$$\n",
    "  **Substituting values:**\n",
    "  \\[ H' = \\frac{32 - 5 + 0}{1} + 1 = 28 \\]\n",
    "  \\[ W' = \\frac{32 - 5 + 0}{1} + 1 = 28 \\]\n",
    "$$\n",
    "**Output Shape After conv1:**\n",
    "```\n",
    "[4, 6, 28, 28]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 3: First Max Pooling Layer (pool)**\n",
    "\n",
    "```python\n",
    "self.pool = nn.MaxPool2d(2,2)\n",
    "```\n",
    "\n",
    "- This applies a **22 max pooling** with **stride=2**.\n",
    "- Pooling reduces each **22 region** to a **single value**, effectively halving height and width.\n",
    "$$\n",
    "  \\[ H' = \\frac{28}{2} = 14, \\quad W' = \\frac{28}{2} = 14 \\]\n",
    "$$\n",
    "**Output Shape After Pooling:**\n",
    "```\n",
    "[4, 6, 14, 14]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 4: Second Convolution Layer (conv2)**\n",
    "\n",
    "```python\n",
    "self.conv2 = nn.Conv2d(6,16,5)\n",
    "```\n",
    "\n",
    "- This applies **16 filters** of size **55** with **stride=1, padding=0**.\n",
    "- Using the formula:\n",
    "$$\n",
    "  \\[ H' = \\frac{14 - 5 + 0}{1} + 1 = 10 \\]\n",
    "  \\[ W' = \\frac{14 - 5 + 0}{1} + 1 = 10 \\]\n",
    "$$\n",
    "**Output Shape After conv2:**\n",
    "```\n",
    "[4, 16, 10, 10]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 5: Second Max Pooling Layer (pool)**\n",
    "\n",
    "```python\n",
    "x = self.pool(F.relu(self.conv2(x)))\n",
    "```\n",
    "\n",
    "- Again, applying **22 max pooling** with **stride=2**:\n",
    "$$\n",
    "  \\[ H' = \\frac{10}{2} = 5, \\quad W' = \\frac{10}{2} = 5 \\]\n",
    "$$\n",
    "**Output Shape After Pooling:**\n",
    "```\n",
    "[4, 16, 5, 5]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 6: Flattening the Tensor (view)**\n",
    "\n",
    "```python\n",
    "x = x.view(-1,16*5*5)\n",
    "```\n",
    "\n",
    "- The tensor is **reshaped (flattened)** to prepare it for the fully connected layers.\n",
    "- The **1655** feature maps are flattened into a vector.\n",
    "\n",
    "  **Since the batch size remains 4, the shape becomes:**\n",
    "```\n",
    "[4, 400]  # (because 1655=400)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 7: First Fully Connected Layer (fc1)**\n",
    "\n",
    "```python\n",
    "self.fc1 = nn.Linear(16*5*5, 120)\n",
    "x = F.relu(self.fc1(x))\n",
    "```\n",
    "\n",
    "- **Input:** `[4, 400]`\n",
    "- **Output:** `[4, 120]` (because `fc1` has 120 neurons)\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 8: Second Fully Connected Layer (fc2)**\n",
    "\n",
    "```python\n",
    "self.fc2 = nn.Linear(120, 84)\n",
    "x = F.relu(self.fc2(x))\n",
    "```\n",
    "\n",
    "- **Input:** `[4, 120]`\n",
    "- **Output:** `[4, 84]` (because `fc2` has 84 neurons)\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 9: Third Fully Connected Layer (fc3)**\n",
    "\n",
    "```python\n",
    "self.fc3 = nn.Linear(84, 10)\n",
    "x = self.fc3(x)\n",
    "```\n",
    "\n",
    "- **Input:** `[4, 84]`\n",
    "- **Output:** `[4, 10]` (because `fc3` has 10 neurons for classification into 10 classes)\n",
    "\n",
    "---\n",
    "\n",
    "## **Final Summary of Tensor Shape Changes**\n",
    "\n",
    "| Layer          | Operation             | Output Shape    |\n",
    "|---------------|----------------------|----------------|\n",
    "| **Input**      | Raw image batch       | `[4, 3, 32, 32]`  |\n",
    "| **Conv1**      | `nn.Conv2d(3,6,5)`   | `[4, 6, 28, 28]`  |\n",
    "| **MaxPool1**   | `nn.MaxPool2d(2,2)`  | `[4, 6, 14, 14]`  |\n",
    "| **Conv2**      | `nn.Conv2d(6,16,5)`  | `[4, 16, 10, 10]` |\n",
    "| **MaxPool2**   | `nn.MaxPool2d(2,2)`  | `[4, 16, 5, 5]`   |\n",
    "| **Flatten**    | `x.view(-1,16*5*5)`  | `[4, 400]`        |\n",
    "| **FC1**        | `nn.Linear(400, 120)` | `[4, 120]`        |\n",
    "| **FC2**        | `nn.Linear(120, 84)`  | `[4, 84]`         |\n",
    "| **FC3 (Output)** | `nn.Linear(84, 10)`  | `[4, 10]`         |\n",
    "\n",
    "---\n",
    "\n",
    "## **Final Output Explanation**\n",
    "- The final output shape is **[4, 10]**, meaning that for each of the 4 images, we have **10 class scores** (for classification).\n",
    "- The output of `fc3` is usually followed by **softmax** (if using classification) to get class probabilities.\n",
    "\n",
    "```python\n",
    "output = F.softmax(x, dim=1)  # Converts scores to probabilities\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  Now you can run this in a Jupyter Notebook and visualize the shape changes! \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "epoch 1/4, step 100/12500, loss = 2.323\n",
      "epoch 1/4, step 200/12500, loss = 2.267\n",
      "epoch 1/4, step 300/12500, loss = 2.341\n",
      "epoch 1/4, step 400/12500, loss = 2.302\n",
      "epoch 1/4, step 500/12500, loss = 2.32\n",
      "epoch 1/4, step 600/12500, loss = 2.256\n",
      "epoch 1/4, step 700/12500, loss = 2.322\n",
      "epoch 1/4, step 800/12500, loss = 2.316\n",
      "epoch 1/4, step 900/12500, loss = 2.289\n",
      "epoch 1/4, step 1000/12500, loss = 2.282\n",
      "epoch 1/4, step 1100/12500, loss = 2.344\n",
      "epoch 1/4, step 1200/12500, loss = 2.332\n",
      "epoch 1/4, step 1300/12500, loss = 2.278\n",
      "epoch 1/4, step 1400/12500, loss = 2.306\n",
      "epoch 1/4, step 1500/12500, loss = 2.315\n",
      "epoch 1/4, step 1600/12500, loss = 2.355\n",
      "epoch 1/4, step 1700/12500, loss = 2.305\n",
      "epoch 1/4, step 1800/12500, loss = 2.301\n",
      "epoch 1/4, step 1900/12500, loss = 2.296\n",
      "epoch 1/4, step 2000/12500, loss = 2.31\n",
      "epoch 1/4, step 2100/12500, loss = 2.337\n",
      "epoch 1/4, step 2200/12500, loss = 2.316\n",
      "epoch 1/4, step 2300/12500, loss = 2.301\n",
      "epoch 1/4, step 2400/12500, loss = 2.311\n",
      "epoch 1/4, step 2500/12500, loss = 2.319\n",
      "epoch 1/4, step 2600/12500, loss = 2.308\n",
      "epoch 1/4, step 2700/12500, loss = 2.293\n",
      "epoch 1/4, step 2800/12500, loss = 2.261\n",
      "epoch 1/4, step 2900/12500, loss = 2.289\n",
      "epoch 1/4, step 3000/12500, loss = 2.308\n",
      "epoch 1/4, step 3100/12500, loss = 2.298\n",
      "epoch 1/4, step 3200/12500, loss = 2.312\n",
      "epoch 1/4, step 3300/12500, loss = 2.304\n",
      "epoch 1/4, step 3400/12500, loss = 2.31\n",
      "epoch 1/4, step 3500/12500, loss = 2.265\n",
      "epoch 1/4, step 3600/12500, loss = 2.28\n",
      "epoch 1/4, step 3700/12500, loss = 2.287\n",
      "epoch 1/4, step 3800/12500, loss = 2.267\n",
      "epoch 1/4, step 3900/12500, loss = 2.298\n",
      "epoch 1/4, step 4000/12500, loss = 2.318\n",
      "epoch 1/4, step 4100/12500, loss = 2.326\n",
      "epoch 1/4, step 4200/12500, loss = 2.268\n",
      "epoch 1/4, step 4300/12500, loss = 2.316\n",
      "epoch 1/4, step 4400/12500, loss = 2.323\n",
      "epoch 1/4, step 4500/12500, loss = 2.295\n",
      "epoch 1/4, step 4600/12500, loss = 2.302\n",
      "epoch 1/4, step 4700/12500, loss = 2.312\n",
      "epoch 1/4, step 4800/12500, loss = 2.303\n",
      "epoch 1/4, step 4900/12500, loss = 2.294\n",
      "epoch 1/4, step 5000/12500, loss = 2.305\n",
      "epoch 1/4, step 5100/12500, loss = 2.306\n",
      "epoch 1/4, step 5200/12500, loss = 2.325\n",
      "epoch 1/4, step 5300/12500, loss = 2.273\n",
      "epoch 1/4, step 5400/12500, loss = 2.292\n",
      "epoch 1/4, step 5500/12500, loss = 2.281\n",
      "epoch 1/4, step 5600/12500, loss = 2.3\n",
      "epoch 1/4, step 5700/12500, loss = 2.313\n",
      "epoch 1/4, step 5800/12500, loss = 2.305\n",
      "epoch 1/4, step 5900/12500, loss = 2.319\n",
      "epoch 1/4, step 6000/12500, loss = 2.294\n",
      "epoch 1/4, step 6100/12500, loss = 2.286\n",
      "epoch 1/4, step 6200/12500, loss = 2.293\n",
      "epoch 1/4, step 6300/12500, loss = 2.3\n",
      "epoch 1/4, step 6400/12500, loss = 2.303\n",
      "epoch 1/4, step 6500/12500, loss = 2.313\n",
      "epoch 1/4, step 6600/12500, loss = 2.317\n",
      "epoch 1/4, step 6700/12500, loss = 2.354\n",
      "epoch 1/4, step 6800/12500, loss = 2.296\n",
      "epoch 1/4, step 6900/12500, loss = 2.323\n",
      "epoch 1/4, step 7000/12500, loss = 2.291\n",
      "epoch 1/4, step 7100/12500, loss = 2.279\n",
      "epoch 1/4, step 7200/12500, loss = 2.312\n",
      "epoch 1/4, step 7300/12500, loss = 2.302\n",
      "epoch 1/4, step 7400/12500, loss = 2.295\n",
      "epoch 1/4, step 7500/12500, loss = 2.291\n",
      "epoch 1/4, step 7600/12500, loss = 2.291\n",
      "epoch 1/4, step 7700/12500, loss = 2.302\n",
      "epoch 1/4, step 7800/12500, loss = 2.288\n",
      "epoch 1/4, step 7900/12500, loss = 2.297\n",
      "epoch 1/4, step 8000/12500, loss = 2.287\n",
      "epoch 1/4, step 8100/12500, loss = 2.264\n",
      "epoch 1/4, step 8200/12500, loss = 2.283\n",
      "epoch 1/4, step 8300/12500, loss = 2.297\n",
      "epoch 1/4, step 8400/12500, loss = 2.299\n",
      "epoch 1/4, step 8500/12500, loss = 2.273\n",
      "epoch 1/4, step 8600/12500, loss = 2.29\n",
      "epoch 1/4, step 8700/12500, loss = 2.33\n",
      "epoch 1/4, step 8800/12500, loss = 2.299\n",
      "epoch 1/4, step 8900/12500, loss = 2.267\n",
      "epoch 1/4, step 9000/12500, loss = 2.277\n",
      "epoch 1/4, step 9100/12500, loss = 2.259\n",
      "epoch 1/4, step 9200/12500, loss = 2.281\n",
      "epoch 1/4, step 9300/12500, loss = 2.27\n",
      "epoch 1/4, step 9400/12500, loss = 2.277\n",
      "epoch 1/4, step 9500/12500, loss = 2.241\n",
      "epoch 1/4, step 9600/12500, loss = 2.288\n",
      "epoch 1/4, step 9700/12500, loss = 2.268\n",
      "epoch 1/4, step 9800/12500, loss = 2.307\n",
      "epoch 1/4, step 9900/12500, loss = 2.248\n",
      "epoch 1/4, step 10000/12500, loss = 2.237\n",
      "epoch 1/4, step 10100/12500, loss = 2.177\n",
      "epoch 1/4, step 10200/12500, loss = 2.226\n",
      "epoch 1/4, step 10300/12500, loss = 2.318\n",
      "epoch 1/4, step 10400/12500, loss = 2.278\n",
      "epoch 1/4, step 10500/12500, loss = 2.318\n",
      "epoch 1/4, step 10600/12500, loss = 2.256\n",
      "epoch 1/4, step 10700/12500, loss = 2.235\n",
      "epoch 1/4, step 10800/12500, loss = 2.242\n",
      "epoch 1/4, step 10900/12500, loss = 2.239\n",
      "epoch 1/4, step 11000/12500, loss = 2.336\n",
      "epoch 1/4, step 11100/12500, loss = 2.174\n",
      "epoch 1/4, step 11200/12500, loss = 2.223\n",
      "epoch 1/4, step 11300/12500, loss = 2.35\n",
      "epoch 1/4, step 11400/12500, loss = 2.268\n",
      "epoch 1/4, step 11500/12500, loss = 2.215\n",
      "epoch 1/4, step 11600/12500, loss = 2.286\n",
      "epoch 1/4, step 11700/12500, loss = 1.864\n",
      "epoch 1/4, step 11800/12500, loss = 2.361\n",
      "epoch 1/4, step 11900/12500, loss = 2.335\n",
      "epoch 1/4, step 12000/12500, loss = 2.107\n",
      "epoch 1/4, step 12100/12500, loss = 2.325\n",
      "epoch 1/4, step 12200/12500, loss = 2.18\n",
      "epoch 1/4, step 12300/12500, loss = 2.064\n",
      "epoch 1/4, step 12400/12500, loss = 2.065\n",
      "epoch 1/4, step 12500/12500, loss = 1.848\n",
      "epoch 2/4, step 100/12500, loss = 2.36\n",
      "epoch 2/4, step 200/12500, loss = 1.849\n",
      "epoch 2/4, step 300/12500, loss = 2.058\n",
      "epoch 2/4, step 400/12500, loss = 2.183\n",
      "epoch 2/4, step 500/12500, loss = 2.095\n",
      "epoch 2/4, step 600/12500, loss = 2.094\n",
      "epoch 2/4, step 700/12500, loss = 1.86\n",
      "epoch 2/4, step 800/12500, loss = 2.486\n",
      "epoch 2/4, step 900/12500, loss = 2.001\n",
      "epoch 2/4, step 1000/12500, loss = 2.432\n",
      "epoch 2/4, step 1100/12500, loss = 1.994\n",
      "epoch 2/4, step 1200/12500, loss = 2.063\n",
      "epoch 2/4, step 1300/12500, loss = 2.52\n",
      "epoch 2/4, step 1400/12500, loss = 2.129\n",
      "epoch 2/4, step 1500/12500, loss = 1.847\n",
      "epoch 2/4, step 1600/12500, loss = 2.556\n",
      "epoch 2/4, step 1700/12500, loss = 2.306\n",
      "epoch 2/4, step 1800/12500, loss = 2.125\n",
      "epoch 2/4, step 1900/12500, loss = 2.089\n",
      "epoch 2/4, step 2000/12500, loss = 2.37\n",
      "epoch 2/4, step 2100/12500, loss = 1.256\n",
      "epoch 2/4, step 2200/12500, loss = 1.714\n",
      "epoch 2/4, step 2300/12500, loss = 1.574\n",
      "epoch 2/4, step 2400/12500, loss = 1.832\n",
      "epoch 2/4, step 2500/12500, loss = 1.757\n",
      "epoch 2/4, step 2600/12500, loss = 2.246\n",
      "epoch 2/4, step 2700/12500, loss = 2.144\n",
      "epoch 2/4, step 2800/12500, loss = 2.135\n",
      "epoch 2/4, step 2900/12500, loss = 1.602\n",
      "epoch 2/4, step 3000/12500, loss = 1.764\n",
      "epoch 2/4, step 3100/12500, loss = 1.742\n",
      "epoch 2/4, step 3200/12500, loss = 2.3\n",
      "epoch 2/4, step 3300/12500, loss = 2.095\n",
      "epoch 2/4, step 3400/12500, loss = 1.659\n",
      "epoch 2/4, step 3500/12500, loss = 2.127\n",
      "epoch 2/4, step 3600/12500, loss = 1.942\n",
      "epoch 2/4, step 3700/12500, loss = 1.83\n",
      "epoch 2/4, step 3800/12500, loss = 1.786\n",
      "epoch 2/4, step 3900/12500, loss = 2.029\n",
      "epoch 2/4, step 4000/12500, loss = 2.258\n",
      "epoch 2/4, step 4100/12500, loss = 1.886\n",
      "epoch 2/4, step 4200/12500, loss = 2.02\n",
      "epoch 2/4, step 4300/12500, loss = 2.245\n",
      "epoch 2/4, step 4400/12500, loss = 2.234\n",
      "epoch 2/4, step 4500/12500, loss = 1.554\n",
      "epoch 2/4, step 4600/12500, loss = 2.231\n",
      "epoch 2/4, step 4700/12500, loss = 2.383\n",
      "epoch 2/4, step 4800/12500, loss = 2.022\n",
      "epoch 2/4, step 4900/12500, loss = 1.538\n",
      "epoch 2/4, step 5000/12500, loss = 2.23\n",
      "epoch 2/4, step 5100/12500, loss = 2.371\n",
      "epoch 2/4, step 5200/12500, loss = 2.413\n",
      "epoch 2/4, step 5300/12500, loss = 1.803\n",
      "epoch 2/4, step 5400/12500, loss = 1.426\n",
      "epoch 2/4, step 5500/12500, loss = 2.414\n",
      "epoch 2/4, step 5600/12500, loss = 1.226\n",
      "epoch 2/4, step 5700/12500, loss = 1.729\n",
      "epoch 2/4, step 5800/12500, loss = 1.814\n",
      "epoch 2/4, step 5900/12500, loss = 2.209\n",
      "epoch 2/4, step 6000/12500, loss = 2.06\n",
      "epoch 2/4, step 6100/12500, loss = 2.234\n",
      "epoch 2/4, step 6200/12500, loss = 2.288\n",
      "epoch 2/4, step 6300/12500, loss = 1.441\n",
      "epoch 2/4, step 6400/12500, loss = 1.277\n",
      "epoch 2/4, step 6500/12500, loss = 2.585\n",
      "epoch 2/4, step 6600/12500, loss = 2.713\n",
      "epoch 2/4, step 6700/12500, loss = 2.001\n",
      "epoch 2/4, step 6800/12500, loss = 1.267\n",
      "epoch 2/4, step 6900/12500, loss = 1.205\n",
      "epoch 2/4, step 7000/12500, loss = 1.695\n",
      "epoch 2/4, step 7100/12500, loss = 1.435\n",
      "epoch 2/4, step 7200/12500, loss = 1.99\n",
      "epoch 2/4, step 7300/12500, loss = 1.573\n",
      "epoch 2/4, step 7400/12500, loss = 2.14\n",
      "epoch 2/4, step 7500/12500, loss = 2.764\n",
      "epoch 2/4, step 7600/12500, loss = 1.641\n",
      "epoch 2/4, step 7700/12500, loss = 2.384\n",
      "epoch 2/4, step 7800/12500, loss = 1.788\n",
      "epoch 2/4, step 7900/12500, loss = 1.541\n",
      "epoch 2/4, step 8000/12500, loss = 2.042\n",
      "epoch 2/4, step 8100/12500, loss = 1.579\n",
      "epoch 2/4, step 8200/12500, loss = 2.166\n",
      "epoch 2/4, step 8300/12500, loss = 2.292\n",
      "epoch 2/4, step 8400/12500, loss = 2.094\n",
      "epoch 2/4, step 8500/12500, loss = 1.887\n",
      "epoch 2/4, step 8600/12500, loss = 1.237\n",
      "epoch 2/4, step 8700/12500, loss = 1.716\n",
      "epoch 2/4, step 8800/12500, loss = 1.794\n",
      "epoch 2/4, step 8900/12500, loss = 1.681\n",
      "epoch 2/4, step 9000/12500, loss = 1.373\n",
      "epoch 2/4, step 9100/12500, loss = 2.313\n",
      "epoch 2/4, step 9200/12500, loss = 1.613\n",
      "epoch 2/4, step 9300/12500, loss = 1.731\n",
      "epoch 2/4, step 9400/12500, loss = 1.908\n",
      "epoch 2/4, step 9500/12500, loss = 1.712\n",
      "epoch 2/4, step 9600/12500, loss = 1.682\n",
      "epoch 2/4, step 9700/12500, loss = 1.723\n",
      "epoch 2/4, step 9800/12500, loss = 1.788\n",
      "epoch 2/4, step 9900/12500, loss = 1.636\n",
      "epoch 2/4, step 10000/12500, loss = 0.965\n",
      "epoch 2/4, step 10100/12500, loss = 2.507\n",
      "epoch 2/4, step 10200/12500, loss = 2.501\n",
      "epoch 2/4, step 10300/12500, loss = 1.69\n",
      "epoch 2/4, step 10400/12500, loss = 2.038\n",
      "epoch 2/4, step 10500/12500, loss = 2.357\n",
      "epoch 2/4, step 10600/12500, loss = 1.644\n",
      "epoch 2/4, step 10700/12500, loss = 2.03\n",
      "epoch 2/4, step 10800/12500, loss = 1.127\n",
      "epoch 2/4, step 10900/12500, loss = 2.349\n",
      "epoch 2/4, step 11000/12500, loss = 1.573\n",
      "epoch 2/4, step 11100/12500, loss = 1.585\n",
      "epoch 2/4, step 11200/12500, loss = 1.703\n",
      "epoch 2/4, step 11300/12500, loss = 2.048\n",
      "epoch 2/4, step 11400/12500, loss = 2.27\n",
      "epoch 2/4, step 11500/12500, loss = 2.261\n",
      "epoch 2/4, step 11600/12500, loss = 1.89\n",
      "epoch 2/4, step 11700/12500, loss = 1.921\n",
      "epoch 2/4, step 11800/12500, loss = 1.466\n",
      "epoch 2/4, step 11900/12500, loss = 2.326\n",
      "epoch 2/4, step 12000/12500, loss = 3.266\n",
      "epoch 2/4, step 12100/12500, loss = 1.669\n",
      "epoch 2/4, step 12200/12500, loss = 2.324\n",
      "epoch 2/4, step 12300/12500, loss = 1.764\n",
      "epoch 2/4, step 12400/12500, loss = 1.306\n",
      "epoch 2/4, step 12500/12500, loss = 1.768\n",
      "epoch 3/4, step 100/12500, loss = 1.063\n",
      "epoch 3/4, step 200/12500, loss = 1.907\n",
      "epoch 3/4, step 300/12500, loss = 1.507\n",
      "epoch 3/4, step 400/12500, loss = 1.324\n",
      "epoch 3/4, step 500/12500, loss = 2.104\n",
      "epoch 3/4, step 600/12500, loss = 1.745\n",
      "epoch 3/4, step 700/12500, loss = 1.56\n",
      "epoch 3/4, step 800/12500, loss = 1.989\n",
      "epoch 3/4, step 900/12500, loss = 1.623\n",
      "epoch 3/4, step 1000/12500, loss = 2.323\n",
      "epoch 3/4, step 1100/12500, loss = 1.292\n",
      "epoch 3/4, step 1200/12500, loss = 1.475\n",
      "epoch 3/4, step 1300/12500, loss = 2.384\n",
      "epoch 3/4, step 1400/12500, loss = 1.494\n",
      "epoch 3/4, step 1500/12500, loss = 3.025\n",
      "epoch 3/4, step 1600/12500, loss = 2.441\n",
      "epoch 3/4, step 1700/12500, loss = 1.365\n",
      "epoch 3/4, step 1800/12500, loss = 1.163\n",
      "epoch 3/4, step 1900/12500, loss = 1.479\n",
      "epoch 3/4, step 2000/12500, loss = 1.846\n",
      "epoch 3/4, step 2100/12500, loss = 1.814\n",
      "epoch 3/4, step 2200/12500, loss = 1.803\n",
      "epoch 3/4, step 2300/12500, loss = 2.288\n",
      "epoch 3/4, step 2400/12500, loss = 1.773\n",
      "epoch 3/4, step 2500/12500, loss = 1.386\n",
      "epoch 3/4, step 2600/12500, loss = 1.435\n",
      "epoch 3/4, step 2700/12500, loss = 1.924\n",
      "epoch 3/4, step 2800/12500, loss = 1.835\n",
      "epoch 3/4, step 2900/12500, loss = 1.333\n",
      "epoch 3/4, step 3000/12500, loss = 2.071\n",
      "epoch 3/4, step 3100/12500, loss = 1.63\n",
      "epoch 3/4, step 3200/12500, loss = 2.591\n",
      "epoch 3/4, step 3300/12500, loss = 1.33\n",
      "epoch 3/4, step 3400/12500, loss = 2.122\n",
      "epoch 3/4, step 3500/12500, loss = 2.256\n",
      "epoch 3/4, step 3600/12500, loss = 1.434\n",
      "epoch 3/4, step 3700/12500, loss = 0.994\n",
      "epoch 3/4, step 3800/12500, loss = 1.892\n",
      "epoch 3/4, step 3900/12500, loss = 1.384\n",
      "epoch 3/4, step 4000/12500, loss = 1.501\n",
      "epoch 3/4, step 4100/12500, loss = 1.318\n",
      "epoch 3/4, step 4200/12500, loss = 2.091\n",
      "epoch 3/4, step 4300/12500, loss = 1.528\n",
      "epoch 3/4, step 4400/12500, loss = 1.736\n",
      "epoch 3/4, step 4500/12500, loss = 2.369\n",
      "epoch 3/4, step 4600/12500, loss = 2.291\n",
      "epoch 3/4, step 4700/12500, loss = 1.087\n",
      "epoch 3/4, step 4800/12500, loss = 1.558\n",
      "epoch 3/4, step 4900/12500, loss = 2.714\n",
      "epoch 3/4, step 5000/12500, loss = 1.745\n",
      "epoch 3/4, step 5100/12500, loss = 2.244\n",
      "epoch 3/4, step 5200/12500, loss = 2.999\n",
      "epoch 3/4, step 5300/12500, loss = 1.829\n",
      "epoch 3/4, step 5400/12500, loss = 1.556\n",
      "epoch 3/4, step 5500/12500, loss = 1.348\n",
      "epoch 3/4, step 5600/12500, loss = 2.024\n",
      "epoch 3/4, step 5700/12500, loss = 0.9089\n",
      "epoch 3/4, step 5800/12500, loss = 1.777\n",
      "epoch 3/4, step 5900/12500, loss = 0.9018\n",
      "epoch 3/4, step 6000/12500, loss = 1.432\n",
      "epoch 3/4, step 6100/12500, loss = 1.384\n",
      "epoch 3/4, step 6200/12500, loss = 1.08\n",
      "epoch 3/4, step 6300/12500, loss = 2.061\n",
      "epoch 3/4, step 6400/12500, loss = 2.258\n",
      "epoch 3/4, step 6500/12500, loss = 1.265\n",
      "epoch 3/4, step 6600/12500, loss = 1.925\n",
      "epoch 3/4, step 6700/12500, loss = 2.065\n",
      "epoch 3/4, step 6800/12500, loss = 1.845\n",
      "epoch 3/4, step 6900/12500, loss = 1.689\n",
      "epoch 3/4, step 7000/12500, loss = 1.618\n",
      "epoch 3/4, step 7100/12500, loss = 1.202\n",
      "epoch 3/4, step 7200/12500, loss = 0.8945\n",
      "epoch 3/4, step 7300/12500, loss = 1.349\n",
      "epoch 3/4, step 7400/12500, loss = 1.739\n",
      "epoch 3/4, step 7500/12500, loss = 2.323\n",
      "epoch 3/4, step 7600/12500, loss = 2.367\n",
      "epoch 3/4, step 7700/12500, loss = 1.241\n",
      "epoch 3/4, step 7800/12500, loss = 1.072\n",
      "epoch 3/4, step 7900/12500, loss = 0.9013\n",
      "epoch 3/4, step 8000/12500, loss = 1.877\n",
      "epoch 3/4, step 8100/12500, loss = 1.343\n",
      "epoch 3/4, step 8200/12500, loss = 2.401\n",
      "epoch 3/4, step 8300/12500, loss = 1.155\n",
      "epoch 3/4, step 8400/12500, loss = 1.79\n",
      "epoch 3/4, step 8500/12500, loss = 1.574\n",
      "epoch 3/4, step 8600/12500, loss = 1.468\n",
      "epoch 3/4, step 8700/12500, loss = 1.438\n",
      "epoch 3/4, step 8800/12500, loss = 1.301\n",
      "epoch 3/4, step 8900/12500, loss = 1.65\n",
      "epoch 3/4, step 9000/12500, loss = 1.045\n",
      "epoch 3/4, step 9100/12500, loss = 1.21\n",
      "epoch 3/4, step 9200/12500, loss = 2.258\n",
      "epoch 3/4, step 9300/12500, loss = 1.859\n",
      "epoch 3/4, step 9400/12500, loss = 1.401\n",
      "epoch 3/4, step 9500/12500, loss = 1.573\n",
      "epoch 3/4, step 9600/12500, loss = 1.669\n",
      "epoch 3/4, step 9700/12500, loss = 1.111\n",
      "epoch 3/4, step 9800/12500, loss = 0.9401\n",
      "epoch 3/4, step 9900/12500, loss = 1.374\n",
      "epoch 3/4, step 10000/12500, loss = 2.024\n",
      "epoch 3/4, step 10100/12500, loss = 1.679\n",
      "epoch 3/4, step 10200/12500, loss = 1.725\n",
      "epoch 3/4, step 10300/12500, loss = 2.31\n",
      "epoch 3/4, step 10400/12500, loss = 1.431\n",
      "epoch 3/4, step 10500/12500, loss = 3.022\n",
      "epoch 3/4, step 10600/12500, loss = 2.037\n",
      "epoch 3/4, step 10700/12500, loss = 2.047\n",
      "epoch 3/4, step 10800/12500, loss = 2.232\n",
      "epoch 3/4, step 10900/12500, loss = 1.025\n",
      "epoch 3/4, step 11000/12500, loss = 1.3\n",
      "epoch 3/4, step 11100/12500, loss = 1.392\n",
      "epoch 3/4, step 11200/12500, loss = 1.475\n",
      "epoch 3/4, step 11300/12500, loss = 1.191\n",
      "epoch 3/4, step 11400/12500, loss = 1.453\n",
      "epoch 3/4, step 11500/12500, loss = 1.04\n",
      "epoch 3/4, step 11600/12500, loss = 0.8186\n",
      "epoch 3/4, step 11700/12500, loss = 2.091\n",
      "epoch 3/4, step 11800/12500, loss = 1.368\n",
      "epoch 3/4, step 11900/12500, loss = 1.029\n",
      "epoch 3/4, step 12000/12500, loss = 1.784\n",
      "epoch 3/4, step 12100/12500, loss = 1.91\n",
      "epoch 3/4, step 12200/12500, loss = 1.913\n",
      "epoch 3/4, step 12300/12500, loss = 1.084\n",
      "epoch 3/4, step 12400/12500, loss = 0.9856\n",
      "epoch 3/4, step 12500/12500, loss = 1.573\n",
      "epoch 4/4, step 100/12500, loss = 1.768\n",
      "epoch 4/4, step 200/12500, loss = 1.621\n",
      "epoch 4/4, step 300/12500, loss = 1.102\n",
      "epoch 4/4, step 400/12500, loss = 2.502\n",
      "epoch 4/4, step 500/12500, loss = 1.732\n",
      "epoch 4/4, step 600/12500, loss = 2.038\n",
      "epoch 4/4, step 700/12500, loss = 1.567\n",
      "epoch 4/4, step 800/12500, loss = 1.597\n",
      "epoch 4/4, step 900/12500, loss = 1.751\n",
      "epoch 4/4, step 1000/12500, loss = 1.111\n",
      "epoch 4/4, step 1100/12500, loss = 1.142\n",
      "epoch 4/4, step 1200/12500, loss = 0.893\n",
      "epoch 4/4, step 1300/12500, loss = 2.172\n",
      "epoch 4/4, step 1400/12500, loss = 1.716\n",
      "epoch 4/4, step 1500/12500, loss = 0.9989\n",
      "epoch 4/4, step 1600/12500, loss = 1.535\n",
      "epoch 4/4, step 1700/12500, loss = 1.32\n",
      "epoch 4/4, step 1800/12500, loss = 1.568\n",
      "epoch 4/4, step 1900/12500, loss = 1.989\n",
      "epoch 4/4, step 2000/12500, loss = 3.199\n",
      "epoch 4/4, step 2100/12500, loss = 2.042\n",
      "epoch 4/4, step 2200/12500, loss = 1.098\n",
      "epoch 4/4, step 2300/12500, loss = 1.051\n",
      "epoch 4/4, step 2400/12500, loss = 1.884\n",
      "epoch 4/4, step 2500/12500, loss = 1.231\n",
      "epoch 4/4, step 2600/12500, loss = 1.628\n",
      "epoch 4/4, step 2700/12500, loss = 1.25\n",
      "epoch 4/4, step 2800/12500, loss = 1.72\n",
      "epoch 4/4, step 2900/12500, loss = 1.811\n",
      "epoch 4/4, step 3000/12500, loss = 1.22\n",
      "epoch 4/4, step 3100/12500, loss = 1.142\n",
      "epoch 4/4, step 3200/12500, loss = 0.9263\n",
      "epoch 4/4, step 3300/12500, loss = 1.494\n",
      "epoch 4/4, step 3400/12500, loss = 1.392\n",
      "epoch 4/4, step 3500/12500, loss = 1.348\n",
      "epoch 4/4, step 3600/12500, loss = 2.246\n",
      "epoch 4/4, step 3700/12500, loss = 1.416\n",
      "epoch 4/4, step 3800/12500, loss = 1.51\n",
      "epoch 4/4, step 3900/12500, loss = 1.331\n",
      "epoch 4/4, step 4000/12500, loss = 0.9137\n",
      "epoch 4/4, step 4100/12500, loss = 1.577\n",
      "epoch 4/4, step 4200/12500, loss = 1.457\n",
      "epoch 4/4, step 4300/12500, loss = 1.023\n",
      "epoch 4/4, step 4400/12500, loss = 1.14\n",
      "epoch 4/4, step 4500/12500, loss = 1.566\n",
      "epoch 4/4, step 4600/12500, loss = 2.57\n",
      "epoch 4/4, step 4700/12500, loss = 1.863\n",
      "epoch 4/4, step 4800/12500, loss = 3.321\n",
      "epoch 4/4, step 4900/12500, loss = 2.035\n",
      "epoch 4/4, step 5000/12500, loss = 1.668\n",
      "epoch 4/4, step 5100/12500, loss = 1.719\n",
      "epoch 4/4, step 5200/12500, loss = 1.44\n",
      "epoch 4/4, step 5300/12500, loss = 1.793\n",
      "epoch 4/4, step 5400/12500, loss = 0.6737\n",
      "epoch 4/4, step 5500/12500, loss = 1.053\n",
      "epoch 4/4, step 5600/12500, loss = 1.718\n",
      "epoch 4/4, step 5700/12500, loss = 0.946\n",
      "epoch 4/4, step 5800/12500, loss = 1.605\n",
      "epoch 4/4, step 5900/12500, loss = 1.198\n",
      "epoch 4/4, step 6000/12500, loss = 1.835\n",
      "epoch 4/4, step 6100/12500, loss = 1.354\n",
      "epoch 4/4, step 6200/12500, loss = 1.528\n",
      "epoch 4/4, step 6300/12500, loss = 1.204\n",
      "epoch 4/4, step 6400/12500, loss = 0.9973\n",
      "epoch 4/4, step 6500/12500, loss = 1.32\n",
      "epoch 4/4, step 6600/12500, loss = 1.493\n",
      "epoch 4/4, step 6700/12500, loss = 1.011\n",
      "epoch 4/4, step 6800/12500, loss = 1.454\n",
      "epoch 4/4, step 6900/12500, loss = 2.031\n",
      "epoch 4/4, step 7000/12500, loss = 1.159\n",
      "epoch 4/4, step 7100/12500, loss = 0.9352\n",
      "epoch 4/4, step 7200/12500, loss = 1.815\n",
      "epoch 4/4, step 7300/12500, loss = 2.023\n",
      "epoch 4/4, step 7400/12500, loss = 1.016\n",
      "epoch 4/4, step 7500/12500, loss = 2.615\n",
      "epoch 4/4, step 7600/12500, loss = 1.289\n",
      "epoch 4/4, step 7700/12500, loss = 1.051\n",
      "epoch 4/4, step 7800/12500, loss = 1.477\n",
      "epoch 4/4, step 7900/12500, loss = 1.395\n",
      "epoch 4/4, step 8000/12500, loss = 0.8876\n",
      "epoch 4/4, step 8100/12500, loss = 1.467\n",
      "epoch 4/4, step 8200/12500, loss = 1.328\n",
      "epoch 4/4, step 8300/12500, loss = 1.889\n",
      "epoch 4/4, step 8400/12500, loss = 0.9661\n",
      "epoch 4/4, step 8500/12500, loss = 1.336\n",
      "epoch 4/4, step 8600/12500, loss = 1.27\n",
      "epoch 4/4, step 8700/12500, loss = 1.531\n",
      "epoch 4/4, step 8800/12500, loss = 1.493\n",
      "epoch 4/4, step 8900/12500, loss = 2.224\n",
      "epoch 4/4, step 9000/12500, loss = 0.9653\n",
      "epoch 4/4, step 9100/12500, loss = 3.23\n",
      "epoch 4/4, step 9200/12500, loss = 2.129\n",
      "epoch 4/4, step 9300/12500, loss = 1.011\n",
      "epoch 4/4, step 9400/12500, loss = 1.825\n",
      "epoch 4/4, step 9500/12500, loss = 1.429\n",
      "epoch 4/4, step 9600/12500, loss = 2.327\n",
      "epoch 4/4, step 9700/12500, loss = 1.066\n",
      "epoch 4/4, step 9800/12500, loss = 1.728\n",
      "epoch 4/4, step 9900/12500, loss = 1.677\n",
      "epoch 4/4, step 10000/12500, loss = 1.597\n",
      "epoch 4/4, step 10100/12500, loss = 1.044\n",
      "epoch 4/4, step 10200/12500, loss = 1.652\n",
      "epoch 4/4, step 10300/12500, loss = 2.976\n",
      "epoch 4/4, step 10400/12500, loss = 1.054\n",
      "epoch 4/4, step 10500/12500, loss = 1.659\n",
      "epoch 4/4, step 10600/12500, loss = 1.539\n",
      "epoch 4/4, step 10700/12500, loss = 1.918\n",
      "epoch 4/4, step 10800/12500, loss = 1.193\n",
      "epoch 4/4, step 10900/12500, loss = 1.772\n",
      "epoch 4/4, step 11000/12500, loss = 1.207\n",
      "epoch 4/4, step 11100/12500, loss = 1.426\n",
      "epoch 4/4, step 11200/12500, loss = 1.059\n",
      "epoch 4/4, step 11300/12500, loss = 2.067\n",
      "epoch 4/4, step 11400/12500, loss = 1.658\n",
      "epoch 4/4, step 11500/12500, loss = 1.689\n",
      "epoch 4/4, step 11600/12500, loss = 1.336\n",
      "epoch 4/4, step 11700/12500, loss = 1.023\n",
      "epoch 4/4, step 11800/12500, loss = 1.138\n",
      "epoch 4/4, step 11900/12500, loss = 1.664\n",
      "epoch 4/4, step 12000/12500, loss = 1.545\n",
      "epoch 4/4, step 12100/12500, loss = 1.796\n",
      "epoch 4/4, step 12200/12500, loss = 1.42\n",
      "epoch 4/4, step 12300/12500, loss = 1.545\n",
      "epoch 4/4, step 12400/12500, loss = 1.035\n",
      "epoch 4/4, step 12500/12500, loss = 1.614\n",
      "Accuracy = 47.79%\n",
      "accuarcy of plane: 43.3%\n",
      "accuarcy of car: 75.0%\n",
      "accuarcy of bird: 31.9%\n",
      "accuarcy of cat: 33.6%\n",
      "accuarcy of deer: 21.0%\n",
      "accuarcy of dog: 37.8%\n",
      "accuarcy of frog: 66.5%\n",
      "accuarcy of horse: 56.0%\n",
      "accuarcy of ship: 61.4%\n",
      "accuarcy of truck: 51.4%\n"
     ]
    }
   ],
   "source": [
    "# creating conv neural net \n",
    "# making a feed forward neural network\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#  device config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# hyper parameters \n",
    "# input_size = 784\n",
    "# hidden_size = 100\n",
    "# num_classes = 10\n",
    "num_epochs = 4\n",
    "batch_size =4\n",
    "learning_rate = 0.001\n",
    "# transform \n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "\n",
    "# minst\n",
    "train_dataset = torchvision.datasets.CIFAR10(root = \"./data\", train =True, transform = transform, download = True)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root = \"./data\", train = False, transform = transform, download = False)\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset= test_dataset, batch_size = batch_size, shuffle = False)\n",
    "classes = (\"plane\",\"car\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\")\n",
    "\n",
    "# examples = iter(train_loader)\n",
    "# samples, labels = next(examples)\n",
    "# print(samples.size(),labels.shape)\n",
    "\n",
    "# output size from a layer depends upon input(a) ,filter(F), padding(p), stride(s) so output size is (a-f+2p)/s +1\n",
    "# if input is 5x5 and filter is 3x3 and padding = 0, stride = 1\n",
    "# output = (5-3+0)/1 +1 = 3 so output will bw 3x3\n",
    "\n",
    "#  here the input size is [4,3,32,32] meaning input has a batch size of 4 and 3 channels it has 32x32 pixels\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,6,5)# this layer have 6 filter each have 2 kernals and kernal size is 5x5\n",
    "        '''Each image in the batch goes through 6 filters.\n",
    "Each filter has 3 separate kernels (one for each input channel).\n",
    "Each kernel slides over its corresponding channel, performing element-wise multiplication.\n",
    "The 3 kernel outputs (one per channel) are summed up to form a single feature map.\n",
    "Since we have 6 filters, we get 6 feature maps as output.\n",
    "The spatial dimensions shrink from 3232 to 2828 due to the 55 kernel.\n",
    "The final output tensor has shape [4,6,28,28].'''\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        self.fc1 = nn.Linear(16*5*5,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1,16*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x \n",
    "        \n",
    "model = ConvNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n",
    "#  training loop \n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs,labels)\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "           print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # value, index\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # Update totals\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] +=1\n",
    "            n_class_samples[label] +=1\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f\"Accuracy = {acc:.2f}%\")\n",
    "\n",
    "for i in range(10):\n",
    "    acc = 100.0 * n_class_correct[i] /  n_class_samples[i]\n",
    "    print(f\"accuarcy of {classes[i]}: {acc}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # transfer learning \n",
    "# # we can train a model to classify car or bike and retrain the last layer to clasify bees and birds\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.optim import lr_scheduler\n",
    "# import numpy as np\n",
    "# import torchvision\n",
    "# from torchvision import datasets, models, transforms\n",
    "# import matplotlib.pyplot as plt\n",
    "# import time \n",
    "# import os\n",
    "# import copy\n",
    "\n",
    "# devie = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# mean = np.array([0485, 0.456, 0.406])\n",
    "# std =  np.array([0.229,0.224, 0.225])\n",
    "\n",
    "# data_transforms = {\n",
    "#     \"train\": transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "#                                  transforms.RandomHorizontalFlip(),\n",
    "#                                  transforms.ToTensor(),\n",
    "#                                  transform.Normalize(mean,std)\n",
    "#                                 ]),\n",
    "#     \"val\": transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "#                                  transforms.RandomHorizontalFlip(),\n",
    "#                                  transforms.ToTensor(),\n",
    "#                                  transform.Normalize(mean,std)\n",
    "#                               ]),\n",
    "# }\n",
    "# #  import data\n",
    "# #  i am just writing this code because i dont have the data with me \n",
    "# data_dir = \"data/hyauvu\"\n",
    "# sets = [\"train\",\"val\"]\n",
    "# image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir,x),data_transforms[x]) \n",
    "#                  for x in [\"train\", \" val\"]}# to access and easily transform in one go datasets.ImageFolder(root, transform=None)\n",
    "# dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size = 4, shuffle = True, num_workers=0) \n",
    "#                for x in [\"train\",\"val\"]} \n",
    "# dataset_sizes = {x:len(image_datasets[x]) for x in [\"train\", \"val\"]}\n",
    "# class_names = image_datasets[\"train\"].classes \n",
    "# print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "#     since = time.time()\n",
    "\n",
    "#     best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#     best_acc = 0.0\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "#         print('-' * 10)\n",
    "\n",
    "#         # Each epoch has a training and validation phase\n",
    "#         for phase in ['train', 'val']:\n",
    "#             if phase == 'train':\n",
    "#                 model.train()  # Set model to training mode\n",
    "#             else:\n",
    "#                 model.eval()   # Set model to evaluate mode\n",
    "\n",
    "#             running_loss = 0.0\n",
    "#             running_corrects = 0\n",
    "\n",
    "#             # Iterate over data.\n",
    "#             for inputs, labels in dataloaders[phase]:\n",
    "#                 inputs = inputs.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "\n",
    "#                 # forward\n",
    "#                 # track history if only in train\n",
    "#                 with torch.set_grad_enabled(phase == 'train'):\n",
    "#                     outputs = model(inputs)\n",
    "#                     _, preds = torch.max(outputs, 1)\n",
    "#                     loss = criterion(outputs, labels)\n",
    "\n",
    "#                     # backward + optimize only if in training phase\n",
    "#                     if phase == 'train':\n",
    "#                         optimizer.zero_grad()\n",
    "#                         loss.backward()\n",
    "#                         optimizer.step()\n",
    "\n",
    "#                 # statistics\n",
    "#                 running_loss += loss.item() * inputs.size(0)\n",
    "#                 running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#             if phase == 'train':\n",
    "#                 scheduler.step()\n",
    "\n",
    "#             epoch_loss = running_loss / dataset_sizes[phase]\n",
    "#             epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "#             print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "#                 phase, epoch_loss, epoch_acc))\n",
    "\n",
    "#             # deep copy the model\n",
    "#             if phase == 'val' and epoch_acc > best_acc:\n",
    "#                 best_acc = epoch_acc\n",
    "#                 best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "#         print()\n",
    "\n",
    "#     time_elapsed = time.time() - since\n",
    "#     print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "#         time_elapsed // 60, time_elapsed % 60))\n",
    "#     print('Best val Acc: {:4f}'.format(best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Finetuning the convnet ####\n",
    "# # Load a pretrained model and reset final fully connected layer.\n",
    "\n",
    "# model = models.resnet18(pretrained=True)\n",
    "# num_ftrs = model.fc.in_features\n",
    "# # Here the size of each output sample is set to 2.\n",
    "# # Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "# model.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "# model = model.to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# # Observe that all parameters are being optimized\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# # StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# # Learning rate scheduling should be applied after optimizers update\n",
    "# # e.g., you should write your code this way:\n",
    "# # for epoch in range(100):\n",
    "# #     train(...)\n",
    "# #     validate(...)\n",
    "# #     scheduler.step()\n",
    "\n",
    "# step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=25)\n",
    "\n",
    "\n",
    "# #### ConvNet as fixed feature extractor ####\n",
    "# # Here, we need to freeze all the network except the final layer.\n",
    "# # We need to set requires_grad == False to freeze the parameters so that the gradients are not computed in backward()\n",
    "# model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "# for param in model_conv.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Parameters of newly constructed modules have requires_grad=True by default\n",
    "# num_ftrs = model_conv.fc.in_features\n",
    "# model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "# model_conv = model_conv.to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# # Observe that only parameters of final layer are being optimized as\n",
    "# # opposed to before.\n",
    "# optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# # Decay LR by a factor of 0.1 every 7 epochs\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "\n",
    "# model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "#                          exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Load TensorBoard extension for Jupyter Notebook\n",
    "from datetime import datetime \n",
    "%load_ext tensorboard\n",
    "\n",
    "# Create a unique log directory to avoid conflicts\n",
    "log_dir = \"runs/mnist_\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "writer = SummaryWriter(log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "epoch 1/2, step 100/938, loss = 0.3837\n",
      "epoch 1/2, step 200/938, loss = 0.3021\n",
      "epoch 1/2, step 300/938, loss = 0.2258\n",
      "epoch 1/2, step 400/938, loss = 0.1348\n",
      "epoch 1/2, step 500/938, loss = 0.1915\n",
      "epoch 1/2, step 600/938, loss = 0.2757\n",
      "epoch 1/2, step 700/938, loss = 0.0927\n",
      "epoch 1/2, step 800/938, loss = 0.0545\n",
      "epoch 1/2, step 900/938, loss = 0.1046\n",
      "epoch 2/2, step 100/938, loss = 0.0669\n",
      "epoch 2/2, step 200/938, loss = 0.0716\n",
      "epoch 2/2, step 300/938, loss = 0.0379\n",
      "epoch 2/2, step 400/938, loss = 0.0290\n",
      "epoch 2/2, step 500/938, loss = 0.0840\n",
      "epoch 2/2, step 600/938, loss = 0.1613\n",
      "epoch 2/2, step 700/938, loss = 0.1350\n",
      "epoch 2/2, step 800/938, loss = 0.0929\n",
      "epoch 2/2, step 900/938, loss = 0.2171\n",
      "Accuracy = 97.46%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# making a feed forward neural network\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sys\n",
    "writer = SummaryWriter(\"runs/mnist\")\n",
    "#  device config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# hyper parameters \n",
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 2\n",
    "batch_size =64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# minst\n",
    "train_dataset = torchvision.datasets.MNIST(root = \"./data\", train =True, transform = transforms.ToTensor(), download = True)\n",
    "test_dataset = torchvision.datasets.MNIST(root = \"./data\", train = False, transform = transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset= test_dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "examples = iter(train_loader)\n",
    "samples, labels = next(examples)\n",
    "print(samples.size(),labels.shape)\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(samples[i][0],cmap = \"gray\")\n",
    "# plt.show()\n",
    "img_grid = torchvision.utils.make_grid(samples)\n",
    "writer.add_image(\"mnist_image\", img_grid)\n",
    "writer.close()\n",
    "# sys.exit()\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_classes):\n",
    "        super(NeuralNet,self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size,hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size,num_classes)\n",
    "    def forward(self,x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out \n",
    "        \n",
    "model = NeuralNet(input_size,hidden_size,num_classes)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
    "samples = samples.to(device)\n",
    "writer.add_graph(model, samples.reshape(-1, 28*28))\n",
    "writer.close()\n",
    "# sys.exit()\n",
    "\n",
    "\n",
    "#  training loop \n",
    "n_total_steps = len(train_loader)\n",
    "running_loss = 0.0\n",
    "running_correct = 0\n",
    "batch_size = train_loader.batch_size  # Get batch size dynamically\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get predictions\n",
    "        running_correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "        # Log every 100 steps\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')\n",
    "\n",
    "            writer.add_scalar(\"training loss\", running_loss / 100, epoch * n_total_steps + i)\n",
    "            running_accuracy = running_correct / (100 * batch_size)  #  Correct accuracy calculation\n",
    "            writer.add_scalar(\"accuracy\", running_accuracy, epoch * n_total_steps + i)\n",
    "\n",
    "            # Reset running loss and correct count for next logging interval\n",
    "            running_loss = 0.0\n",
    "            running_correct = 0\n",
    "\n",
    "            \n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # value, index\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # Update totals\n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f\"Accuracy = {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -7.7697,   5.4145,  -3.2154,  -1.0246,  -5.0071,  -9.7359,  -7.4430,\n",
       "           0.4890,  -2.4342,  -5.1512],\n",
       "        [ -6.7906,  -1.1680,   9.0384,   2.0006, -17.4761,  -8.1463, -15.0140,\n",
       "          -0.2048,   0.7796,  -6.9645],\n",
       "        [ -8.9091,  -4.6189,   0.4966,   6.8220, -16.2527,  -2.6915, -17.4786,\n",
       "          -6.2825,   0.7402,  -3.2451],\n",
       "        [ -9.0834,  -9.5313,  -5.5198,  -6.8558,  11.5520,  -6.2931,  -5.3191,\n",
       "          -2.2397,  -2.4007,   0.9939],\n",
       "        [ -6.5265,  -9.9933, -10.2689,   1.0187,  -8.8871,  10.7741, -12.4940,\n",
       "         -10.3786,  -1.3112,  -4.8622],\n",
       "        [ -2.1357,  -8.2540,   1.2573,  -3.8730,  -2.0739,  -1.6866,  11.7585,\n",
       "         -11.3157,  -4.4694, -12.7137],\n",
       "        [ -6.3331,  -4.9699,   2.4999,   2.6198, -13.1311, -11.1928, -18.8531,\n",
       "           9.8988,  -2.4841,  -2.9238],\n",
       "        [ -0.7374, -10.0532,  -0.8336,   3.4233, -12.2419,  -3.0842, -10.6804,\n",
       "          -6.1248,   7.5527,  -6.0853],\n",
       "        [ -3.8302, -12.2052,  -3.0111,  -6.3443,   4.2233,  -6.5423,  -6.3165,\n",
       "          -1.2342,  -3.7394,   6.0171],\n",
       "        [  8.7170, -13.8930,  -0.5944,  -4.2019,  -7.8578,  -2.2099,   1.1044,\n",
       "         -10.7347,   3.6039,  -3.8483],\n",
       "        [ -7.5813,   5.6718,  -1.4253,  -4.7273,  -4.6271,  -7.6672,  -3.7258,\n",
       "          -2.6159,  -2.0033,  -8.9804],\n",
       "        [ -9.4993,   2.0779,  11.6232,   3.0185, -20.2900,  -7.3601, -16.3040,\n",
       "          -1.7871,   0.1241, -12.3778],\n",
       "        [ -9.9427,  -3.5380,  -0.6418,  11.1017, -17.6691,   1.5252, -19.2110,\n",
       "          -6.8115,  -3.3633,  -1.9714],\n",
       "        [ -8.2921, -10.7286,  -8.1772,  -4.9477,   9.6480,  -7.6147,  -5.1593,\n",
       "          -1.0291,  -0.1038,   1.8040],\n",
       "        [ -3.2653,  -8.0084,  -9.0457,  -2.7746,  -6.1450,   9.1992,  -2.7751,\n",
       "          -5.2743,   0.7720,  -8.8854],\n",
       "        [ -2.2752,  -8.7905,  -1.4391,  -6.2768,  -0.7120,  -0.4874,  12.6221,\n",
       "         -10.2233,  -7.1925,  -8.8088]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -7.7697,   5.4145,  -3.2154,  -1.0246,  -5.0071,  -9.7359,  -7.4430,\n",
       "           0.4890,  -2.4342,  -5.1512],\n",
       "        [ -6.7906,  -1.1680,   9.0384,   2.0006, -17.4761,  -8.1463, -15.0140,\n",
       "          -0.2048,   0.7796,  -6.9645],\n",
       "        [ -8.9091,  -4.6189,   0.4966,   6.8220, -16.2527,  -2.6915, -17.4786,\n",
       "          -6.2825,   0.7402,  -3.2451],\n",
       "        [ -9.0834,  -9.5313,  -5.5198,  -6.8558,  11.5520,  -6.2931,  -5.3191,\n",
       "          -2.2397,  -2.4007,   0.9939],\n",
       "        [ -6.5265,  -9.9933, -10.2689,   1.0187,  -8.8871,  10.7741, -12.4940,\n",
       "         -10.3786,  -1.3112,  -4.8622],\n",
       "        [ -2.1357,  -8.2540,   1.2573,  -3.8730,  -2.0739,  -1.6866,  11.7585,\n",
       "         -11.3157,  -4.4694, -12.7137],\n",
       "        [ -6.3331,  -4.9699,   2.4999,   2.6198, -13.1311, -11.1928, -18.8531,\n",
       "           9.8988,  -2.4841,  -2.9238],\n",
       "        [ -0.7374, -10.0532,  -0.8336,   3.4233, -12.2419,  -3.0842, -10.6804,\n",
       "          -6.1248,   7.5527,  -6.0853],\n",
       "        [ -3.8302, -12.2052,  -3.0111,  -6.3443,   4.2233,  -6.5423,  -6.3165,\n",
       "          -1.2342,  -3.7394,   6.0171],\n",
       "        [  8.7170, -13.8930,  -0.5944,  -4.2019,  -7.8578,  -2.2099,   1.1044,\n",
       "         -10.7347,   3.6039,  -3.8483],\n",
       "        [ -7.5813,   5.6718,  -1.4253,  -4.7273,  -4.6271,  -7.6672,  -3.7258,\n",
       "          -2.6159,  -2.0033,  -8.9804],\n",
       "        [ -9.4993,   2.0779,  11.6232,   3.0185, -20.2900,  -7.3601, -16.3040,\n",
       "          -1.7871,   0.1241, -12.3778],\n",
       "        [ -9.9427,  -3.5380,  -0.6418,  11.1017, -17.6691,   1.5252, -19.2110,\n",
       "          -6.8115,  -3.3633,  -1.9714],\n",
       "        [ -8.2921, -10.7286,  -8.1772,  -4.9477,   9.6480,  -7.6147,  -5.1593,\n",
       "          -1.0291,  -0.1038,   1.8040],\n",
       "        [ -3.2653,  -8.0084,  -9.0457,  -2.7746,  -6.1450,   9.1992,  -2.7751,\n",
       "          -5.2743,   0.7720,  -8.8854],\n",
       "        [ -2.2752,  -8.7905,  -1.4391,  -6.2768,  -0.7120,  -0.4874,  12.6221,\n",
       "         -10.2233,  -7.1925,  -8.8088]], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 25314), started 2 days, 2:23:59 ago. (Use '!kill 25314' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1406c676ab4a2a6e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1406c676ab4a2a6e\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#  LAZY OPTION\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#  TO SAVE\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model, \u001b[43mPATH\u001b[49m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# TO LOAD A MODEL \u001b[39;00m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(PATH)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PATH' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#  LAZY OPTION\n",
    "#  TO SAVE\n",
    "torch.save(model, PATH)\n",
    "\n",
    "# TO LOAD A MODEL \n",
    "model = torch.load(PATH)\n",
    "model.eval()\n",
    "\n",
    "#  recommended method\n",
    "# save a state \n",
    "torch.save(model.state_dict(),path)\n",
    "# model must be created again with parameters\n",
    "model = model(*agrs,**kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
